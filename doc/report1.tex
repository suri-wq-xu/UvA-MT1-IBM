\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{amsmath,amssymb}

\newcommand\BibTeX{B{\sc ib}\TeX}
\aclfinalcopy

\title{NLP 2 -- Project 1}
\author{Pepijn Kokke \and Edwin Odijk \and Thijs Scheepers}

\date{}

\begin{document}

\maketitle

\begin{abstract}
%we translate stuff
%we use IBM Models 1 \& 2
%improvements used
%experiments done
%results
%conclusion
\end{abstract}

\section{Introduction}
%we translated using IBM models 1 & 2
%mention shortcomings of these models (such as the "ook -> as well" example, where translation can't happen both ways (should be confirmed though, maybe we're making an oversight))
%we made some improvements
Machine translation remains a difficult problem to this day. While it's still not perfect, many steps have been taken over the years to bring to where we are today. (ref\_brown) introduced the IBM models 1 through 5 back in 1993 to deal with the problem of alignment: finding the words in a target language that correspond to the words in a source language. Although the models by themselves are insufficient for translation in our current time, the word alignments are still valuable. We implement IBM models 1 and 2 to do this alignment and investigate several improvements by (ref\_moore) as well as some initialisations. In the remainder of this report, we will refer to the source language as French and the target language as English, as this is not only the terminology used by (ref\_brown), but also refers to the actual languages we used for alignment. However, as we describe a generative model, we look at how our French source words were generated from the English words. %something about the parallel corpus here with a source?

\section{Model}
%short summary IBM 1 \& 2
%param optim
%describe choice and implementation of each improvement (#NOTE: V is not 100000 but rather the number of unique words in english)
%mention shortcomings
IBM model 1 is the simplest of the two we implemented, but nevertheless useful as its output can be used to initialise the other model. The model tries to, for each word in each French sentence, find the word in the corresponding English sentence that it was generated from. This link between two corresponding words carries a weight, and assigning these words to eachother forms an alignment.

To get a more in-depth definition of the model, let:
\begin{itemize}
\item l = English sentence length
\item m = French sentence length
\item i = position of an English word in a sentence
\item j = position of a French word in a sentence
\item aj = position of an English word that the j'th French word is aligned to
\end{itemize}

Ideally, we would want a $p(f|e)$ to translate with, but as this is hard to compute, we get the alignments involved and end up with
\begin{align}
p(f,a|e,m &= p(a|e,m)p(f|a,e,m)
\end{align} 
This means it assigns an alignment with probability
\begin{align}
p(a|e,m) &= \frac{1}{(l+1)^m}
\end{align}
and a french word with probability
\begin{align}
p(f|a,e,m) &= \prod_{j}^{m}t(f_j | e_{a_j})
\end{align}
which is the translation probability of the French word $f_j$ being translated from the English word it is aligned to.

We now wish to optimize this $t(f_j | e_{a_j})$ using the EM algorithm. To do so, we first perform the \textbf{E} step:\\
For each sentence in the corpus, for each English and French word in that sentence we compute the counts:
\begin{align}
c(e_j^k , f_i^k ) &\leftarrow c(e_j^k , f_i^k ) + \delta(k,i,j)\\
c(e_j^k ) &\leftarrow c( e_j^k ) + \delta(k,i,j)
\end{align}
where the $\delta$ function returns 1 if the j'th word is aligned to the i'th word in the k'th sentence, otherwise 0. We then find $t(f_j | e_{a_j})$ in the \textbf{M} step:\\
\begin{align}
t(f|e) &= \frac{c(e,f)}{c(e}
\end{align}
by looping over all combinations.

\section{Experiments}
%mention data (train set, test set) #NOTE: test set is not yet provided to us, still greyed out on website. If we never get any, we'll have to split the train data
%mention both models will be described separately

\subsection{IBM model1}
%EM on train set
%improvements on IBM model 1
%Viterbi alignments (= most likely alignments, argmax a) and computing AER (yet to be provided)

\subsection{IBM model 2}
%optimisation with different model parameters: (NOTE: literally from the project description)
% - uniform init
% - random init (THREE times)
% - initialising lexical parameters with results from IBM model 1 (=unique words recognised)
% compute AER again and compare with IBM model 1

\section{Results}
%mention both models will be described separately

\subsection{IBM model 1}
% log likelihood change over iterations plot
%"In your report, you should also consider the limitations of Model 1 as described by Moore (2004), and find examples in your output to illustrate these limitations."
%viterbi alignments & AER results

\subsection{IBM model 2}
% for all FIVE parameter initializations (five because random is run three times), plot log likelihood growth over iterations
% AER results, compare with IBM model 1 results

\section{Conclusion}
% likely points of interest:
% - model 2 will (should) outperform model 1
% - improvements on model 1
% - initialising model 2 with model 1's lexical types is likely to outperform the uniform/random initialisation

\end{document}
